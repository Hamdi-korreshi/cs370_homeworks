{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "import os\n",
    "\n",
    "\n",
    "def grab_videos(links):\n",
    "    # Grab the current directory\n",
    "    curr_dir = os.getcwd()\n",
    "\n",
    "\n",
    "    # Read video links from a file\n",
    "    for link in links:\n",
    "        # Create the YouTube object to access everything\n",
    "        video = YouTube(link)\n",
    "\n",
    "        # Create the captions file name and the folder name\n",
    "        captions = video.title + \" Captions\"\n",
    "        folder = \"Videos\"\n",
    "\n",
    "        # Make the path for captions to be written to and video downloaded into\n",
    "        folder_path = os.path.join(curr_dir, folder)\n",
    "\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "\n",
    "        \n",
    "        video.streams.get_by_itag(22).download(output_path=folder_path)\n",
    "        \n",
    "        try:\n",
    "            caption = video.captions['en']\n",
    "        except:\n",
    "            caption = video.captions['a.en']\n",
    "        # Modify to make the file inside the folder\n",
    "        write_captions = os.path.join(folder, captions + \".txt\")\n",
    "        file1 = open(write_captions, \"w\")\n",
    "\n",
    "        # Make the file in a readable format\n",
    "        file1.write(caption.generate_srt_captions())\n",
    "        file1.close()\n",
    "        print(caption)\n",
    "\n",
    "        # Download video to the new folder\n",
    "\n",
    "    print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python in /home/hamdi/.venv/lib/python3.10/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/hamdi/.venv/lib/python3.10/site-packages (from opencv-contrib-python) (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Caption lang=\"English (auto-generated)\" code=\"a.en\">\n",
      "<Caption lang=\"English\" code=\"en\">\n",
      "<Caption lang=\"English\" code=\"en\">\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "links = ['https://www.youtube.com/watch?v=wbWRWeVe1XE','https://www.youtube.com/watch?v=FlJoBhLnqko','https://www.youtube.com/watch?v=Y-bVwPRy_no']\n",
    "grab_videos(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and resizing: What Does High-Quality Preschool Look Like  NPR Ed.mp4\n",
      "46 frames saved \n",
      "Loading and resizing: Why It’s Usually Hotter In A City  Lets Talk  NPR.mp4\n",
      "67 frames saved \n",
      "Loading and resizing: How Green Roofs Can Help Cities  NPR.mp4\n",
      "98 frames saved \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "folder = \"Videos\"\n",
    "prep = \"Frames\"\n",
    "all_count = 0\n",
    "if not os.path.exists(prep):\n",
    "    os.mkdir(prep)\n",
    "    print(\"MADE\")\n",
    "for video in os.listdir(folder):\n",
    "    if video.endswith('.mp4'):\n",
    "        print(\"Loading and resizing:\" , video)\n",
    "        video_path = os.path.join(folder, video)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        count = 0\n",
    "        # read frame loop\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "        # Break the loop if no more frames are available\n",
    "            if not ret:\n",
    "                break\n",
    "            # every frame 90th frame because its gonna be way too big othewise and now we are grabbing ever 3 seconds or so\n",
    "            # int is needed so my frames arent 257.9888887877678\n",
    "            if count % 180 == 0:\n",
    "                all_count +=1\n",
    "                frame_filename = os.path.join(prep, f\"{video} || {count}.jpg\")\n",
    "                resized_frame = cv2.resize(frame, (224, 224))\n",
    "                cv2.imwrite(frame_filename, frame)\n",
    "                count+=1\n",
    "            count += 1\n",
    "        # you the release otherwise it courrpts you're docker and or whole ipynb fil \n",
    "        cap.release()\n",
    "        print(all_count, \"frames saved \")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tab_struct = pd.DataFrame(columns=['vidId', 'frameNum', 'timestamp', 'detectedObjId', 'detectedObjClass', 'confidence', 'bbox info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamdi/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hamdi/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.8123e-01,  6.8044e-01, -2.9265e-01, -6.8197e-01,  8.1927e-01,\n",
      "          6.8518e-01, -1.0506e+00, -5.8375e-01, -1.2456e+00, -3.3138e-01,\n",
      "         -1.6289e-01, -9.0133e-01, -1.2978e+00, -9.6438e-01, -1.2784e+00,\n",
      "         -7.7441e-01, -9.7797e-01, -9.5174e-01, -1.0102e+00, -1.1468e+00,\n",
      "         -6.0078e-01, -2.7654e-01,  6.4864e-01, -5.1372e-01, -6.8797e-01,\n",
      "         -3.0467e-01, -5.8874e-01, -2.6677e-01,  2.2034e-01,  3.9059e-01,\n",
      "         -8.5710e-01,  1.5026e+00, -4.4904e-02, -3.5381e-01,  1.0047e+00,\n",
      "         -1.7184e+00, -6.8472e-01, -1.5811e+00,  1.8594e+00, -2.5049e-01,\n",
      "         -2.8388e-01, -1.3615e+00, -4.8172e-01, -4.9796e-01, -5.0195e-01,\n",
      "         -2.4162e-01, -1.4636e-01,  8.8876e-02, -8.0013e-01, -1.3225e+00,\n",
      "         -5.7882e-01,  1.2372e-01, -7.1666e-02,  8.8554e-02, -5.0157e-02,\n",
      "          4.6387e-01, -1.2486e+00, -1.8409e+00, -1.1785e+00,  2.1635e+00,\n",
      "          1.4166e+00, -5.2569e-01,  1.2401e-01,  9.7554e-01,  5.9697e-01,\n",
      "         -9.6538e-02,  5.5509e-01, -8.5262e-01,  5.0821e-01,  1.0448e+00,\n",
      "         -6.9215e-01,  1.9470e+00, -1.2225e+00,  1.7346e+00, -1.3548e+00,\n",
      "         -7.4530e-02, -1.8565e-01,  1.3398e-01,  1.0341e+00,  1.0487e+00,\n",
      "         -6.9019e-01,  5.4362e-02, -9.6716e-01, -1.9617e+00, -1.1268e+00,\n",
      "         -1.2987e+00, -1.1047e+00, -8.2208e-01, -3.8211e-01,  1.5001e-01,\n",
      "         -8.9548e-01, -1.5136e+00, -2.1201e-01, -5.7346e-01,  3.2881e-01,\n",
      "         -1.6828e+00, -1.8913e-01, -2.4293e+00, -1.3010e+00, -6.1288e-01,\n",
      "         -4.5658e-01, -1.0909e+00, -1.3512e+00,  5.6210e-01, -9.0040e-01,\n",
      "         -1.7684e+00, -7.6069e-01,  2.0228e+00,  1.6051e-01, -7.6489e-01,\n",
      "          3.2854e-02,  4.0707e+00,  1.3006e+00,  1.1179e+00, -9.8906e-02,\n",
      "         -2.0091e-01, -1.5816e-01,  2.2043e+00, -1.6446e+00, -1.0606e+00,\n",
      "         -3.2308e-01, -1.2776e+00, -9.7856e-01, -5.6973e-01, -5.1367e-01,\n",
      "         -2.3909e-01,  1.7859e+00, -1.1133e-01,  1.2586e-01,  1.4674e-01,\n",
      "         -3.2925e-01, -1.1624e+00,  4.2682e-01, -1.6352e+00, -1.2922e+00,\n",
      "         -9.8949e-01, -1.7077e+00, -1.4386e+00, -1.7558e+00, -1.6751e+00,\n",
      "         -1.0676e+00, -9.6219e-01, -1.8413e+00, -8.8778e-01, -7.3772e-01,\n",
      "         -6.8145e-01, -1.0441e+00, -2.9699e-01, -8.7979e-01, -6.7848e-02,\n",
      "         -1.4307e+00,  7.9127e-02, -2.5093e-01,  2.3708e-01, -9.4535e-01,\n",
      "         -6.4048e-01, -1.0004e+00, -1.2366e+00, -5.5230e-01, -8.2378e-01,\n",
      "         -1.4900e+00, -3.6784e-01, -4.1474e-01,  1.8728e-01, -6.5759e-01,\n",
      "         -5.1085e-01, -7.3308e-01, -6.4227e-01, -1.1189e+00, -8.8020e-01,\n",
      "         -1.1261e+00, -6.7158e-01, -7.3196e-01, -2.2130e-01, -1.9282e+00,\n",
      "         -1.0637e+00, -1.1275e+00, -1.0268e+00,  3.7887e-01, -1.0562e+00,\n",
      "         -9.2072e-01, -6.3348e-01, -1.2342e+00, -1.1612e+00, -8.7376e-01,\n",
      "         -4.0101e-01, -1.1098e+00,  2.3022e-02, -9.3194e-01, -4.3772e-01,\n",
      "         -9.9138e-02, -7.3103e-01, -1.6470e+00, -1.0791e+00, -8.6295e-01,\n",
      "          3.0892e-01, -9.3693e-01, -2.0971e+00, -1.4194e+00, -9.7277e-01,\n",
      "         -1.0947e+00, -1.5608e+00, -1.4250e+00,  2.2639e-02,  4.9199e-01,\n",
      "         -2.1802e+00, -1.3455e+00, -5.0777e-01, -4.2413e-01, -1.2499e+00,\n",
      "         -1.8516e+00, -1.2252e+00, -1.2586e+00, -1.1055e+00, -1.2906e+00,\n",
      "         -8.0874e-01, -5.4956e-02, -1.0138e+00, -1.4207e+00, -7.1063e-01,\n",
      "         -2.4851e-01, -6.3039e-01, -3.7012e-01, -1.5161e-01, -3.3526e-01,\n",
      "         -1.4314e+00, -1.6999e+00, -1.6687e+00, -5.9773e-01, -9.5059e-01,\n",
      "         -1.3464e+00, -8.1936e-01, -5.2400e-01, -7.1579e-01, -1.0966e+00,\n",
      "         -7.8404e-01, -4.9479e-01, -9.9560e-01, -8.6537e-01, -5.4469e-01,\n",
      "         -1.3875e+00, -8.5499e-01, -1.1248e+00, -1.4771e+00, -8.8532e-01,\n",
      "         -8.0489e-01, -5.2844e-01, -9.5244e-01, -1.1020e+00, -1.3656e+00,\n",
      "         -5.9945e-02, -2.0085e-01, -4.5506e-01, -9.2252e-01,  1.1822e-01,\n",
      "         -2.4286e+00, -1.4449e+00, -5.9221e-01, -7.0139e-01, -8.4813e-01,\n",
      "         -3.8380e-01, -1.4584e+00, -1.4844e-01, -1.3148e+00, -3.9139e-01,\n",
      "         -6.2561e-01, -4.9837e-01, -1.2349e+00, -1.2247e+00, -4.0549e-01,\n",
      "          1.3159e-01, -5.9353e-01, -5.0773e-01, -1.3073e+00, -6.6448e-01,\n",
      "         -9.6634e-01, -1.1513e-01,  6.8032e-01, -6.3218e-01, -1.3955e-01,\n",
      "         -1.9608e-01,  2.4327e-01, -5.4448e-01,  8.9172e-01, -1.7035e-02,\n",
      "          1.0189e+00, -8.3410e-01, -8.4703e-01,  1.6612e-01, -6.4311e-01,\n",
      "         -1.1739e+00, -5.4686e-01, -1.1531e+00, -1.3870e-01, -6.1948e-01,\n",
      "         -1.3772e+00, -6.2490e-01, -6.4827e-01, -7.2138e-01,  2.1066e-01,\n",
      "         -6.4527e-01,  5.5334e-01, -1.0787e+00,  2.3071e-01, -6.5275e-01,\n",
      "         -1.0507e+00,  7.6765e-01,  1.0320e+00,  9.9259e-01, -8.6573e-01,\n",
      "          8.5797e-01, -1.0332e-01,  7.2594e-01,  6.4486e-01,  1.0854e+00,\n",
      "          1.1394e+00,  9.7011e-02,  6.5375e-01,  1.3465e+00,  3.6358e-01,\n",
      "         -3.2314e-01, -1.6068e+00, -1.9574e+00, -9.6804e-01,  2.2476e-02,\n",
      "         -1.1605e+00,  7.6016e-01,  8.9828e-02,  2.4152e-01, -1.4339e+00,\n",
      "         -1.3590e+00, -7.8232e-02, -4.0022e-01, -5.7609e-01, -1.3822e+00,\n",
      "         -1.2065e+00, -1.6253e+00, -1.0772e+00, -7.4157e-01, -1.5459e+00,\n",
      "         -6.9655e-01, -8.9071e-01, -1.4493e+00, -1.7873e+00, -1.8703e+00,\n",
      "         -6.6915e-01, -1.6305e+00, -1.0512e+00, -9.5195e-01, -8.8779e-01,\n",
      "         -4.0599e-01, -5.7736e-01, -1.2022e+00, -6.5317e-01, -4.0412e-01,\n",
      "         -8.4374e-01,  5.1552e-02, -9.2902e-01, -1.7346e-01, -2.0247e-01,\n",
      "         -1.1277e+00,  1.9253e-01,  7.1039e-02, -3.9825e-01, -3.5043e-03,\n",
      "         -7.5833e-01, -1.0448e+00, -1.4241e-01, -1.3257e+00, -1.1853e+00,\n",
      "         -1.2354e+00, -1.0014e+00, -1.7273e+00, -1.0032e+00, -1.1135e+00,\n",
      "         -1.2230e+00, -1.6213e-01, -3.4745e-01, -4.6160e-01, -1.7640e+00,\n",
      "         -3.9859e-01, -1.2193e+00, -9.7955e-01, -9.2165e-01, -9.0724e-01,\n",
      "         -6.0952e-01, -1.5333e+00, -9.9461e-01, -1.2534e+00,  2.2977e-01,\n",
      "         -5.3356e-01, -1.1728e+00, -4.0716e-02, -9.9189e-01, -4.5189e-01,\n",
      "         -1.9723e-01, -5.2243e-01,  9.5766e-02,  4.0321e-01,  2.0278e+00,\n",
      "         -1.1035e-01,  1.7605e+00,  1.6811e+00, -4.0735e-01, -1.0245e+00,\n",
      "          1.7795e-01, -1.0902e+00, -9.0584e-01, -1.3116e+00,  2.0890e+00,\n",
      "         -1.5148e+00, -5.3506e-01, -5.2190e-01,  1.7110e+00, -6.2283e-01,\n",
      "         -1.4986e+00,  2.7163e+00,  5.5052e-01,  1.9336e+00,  2.1002e+00,\n",
      "          2.2063e-01,  1.4135e+00,  4.7511e-01,  6.9820e-01, -2.5404e-02,\n",
      "         -5.1473e-02,  2.2388e-01,  7.4204e-01, -7.8479e-01,  1.6857e+00,\n",
      "          3.0893e-02, -9.2876e-01,  1.1759e+00,  3.5581e-01, -3.2338e-01,\n",
      "          8.8703e-01, -9.5809e-01,  1.0969e+00,  2.1453e+00,  3.2607e-01,\n",
      "          8.0427e-01,  1.4215e+00,  1.2352e+00,  3.0195e-01, -6.0794e-01,\n",
      "         -3.2930e-04,  1.2924e+00,  6.8926e-01, -3.5268e-01, -9.0756e-01,\n",
      "         -1.6187e+00,  1.6772e+00, -5.7117e-01,  3.5071e-02, -1.8967e+00,\n",
      "          3.5287e-01,  7.0969e-01,  1.7960e+00, -1.1405e+00,  9.0366e-01,\n",
      "         -1.1955e+00,  7.1333e-01,  1.2134e+00,  6.6168e-01,  1.7113e+00,\n",
      "          1.7221e+00, -6.3958e-01, -1.3012e+00,  3.0857e-01,  1.3023e+00,\n",
      "          2.8379e+00, -1.6097e-01, -1.1768e+00,  2.0720e+00, -7.3019e-01,\n",
      "          4.3581e-01, -1.8588e+00,  2.0465e-01,  3.5029e-01,  1.5516e+00,\n",
      "          5.7432e-01,  1.7394e+00,  9.7862e-01, -2.1846e-01, -1.0531e+00,\n",
      "          1.1013e+00,  1.3691e+00,  1.7678e+00,  2.0807e+00, -8.1292e-01,\n",
      "          6.4812e-01,  1.1659e+00,  2.3383e-01, -4.6957e-01,  5.9284e-01,\n",
      "         -9.3409e-01, -4.4016e-01,  5.8284e-01,  1.2239e+00,  2.8664e+00,\n",
      "         -3.0884e-01,  9.0139e-01, -9.3218e-03,  1.7201e+00,  1.6481e+00,\n",
      "          2.0281e-01,  9.0668e-01,  1.5652e+00,  1.2994e+00, -2.0622e+00,\n",
      "         -6.6082e-02, -1.1450e+00,  1.9811e+00,  2.3710e+00, -3.5765e-01,\n",
      "          1.0695e+00,  1.5545e-01,  1.0686e+00,  1.4138e-01,  1.1019e-01,\n",
      "         -1.0678e+00, -9.2572e-01, -2.0816e-01,  9.6132e-01,  8.8444e-03,\n",
      "         -1.2009e+00,  2.5835e-01,  1.4939e+00,  9.5958e-01,  9.0137e-01,\n",
      "          3.3948e+00,  1.8050e+00, -1.2339e+00, -7.5866e-01,  9.4943e-01,\n",
      "          2.9001e-01, -9.9680e-01, -1.6249e+00,  3.6140e-01,  1.2313e+00,\n",
      "         -3.3092e-01,  9.8200e-01,  2.2972e+00,  1.1337e+00,  1.0226e-01,\n",
      "          3.8682e-01,  1.6591e+00, -1.7606e+00, -1.0438e+00,  1.3404e+00,\n",
      "          3.7355e-01,  1.3657e+00,  1.2467e+00,  3.7964e-01, -1.2442e+00,\n",
      "         -1.1446e+00,  5.3088e-02,  8.0940e-01,  1.3979e+00,  7.6742e-01,\n",
      "          3.4831e-01, -4.3758e-01,  9.1953e-01,  1.5107e+00, -1.3407e+00,\n",
      "         -1.0144e+00,  8.0333e-02,  1.0970e+00,  1.6734e-01, -8.6402e-01,\n",
      "          1.9122e+00, -5.5808e-01,  1.6223e+00, -4.2850e-01,  2.3516e+00,\n",
      "         -1.4711e+00, -1.0721e+00,  9.8230e-01,  4.3294e-01,  1.3296e+00,\n",
      "         -4.0400e-01, -6.4054e-01, -9.4563e-01,  1.9357e+00,  1.3285e+00,\n",
      "          1.4272e+00, -1.2767e+00,  1.7647e+00, -2.0888e-01, -2.4626e-01,\n",
      "          4.0632e-01,  8.2376e-01,  7.5049e-01,  2.1553e+00,  1.0150e+00,\n",
      "         -1.1829e+00,  1.9356e+00,  1.0492e-01,  4.4722e-01,  8.3115e-01,\n",
      "          2.0032e+00,  4.1414e-01,  1.7729e+00, -1.9330e+00,  3.1120e+00,\n",
      "          1.5741e+00,  1.5903e+00,  2.1294e+00,  8.1666e-01, -9.4626e-01,\n",
      "          5.4930e-01, -7.9026e-02, -1.1027e+00,  9.4777e-01,  1.1464e-01,\n",
      "          9.2407e-01,  2.2070e+00,  5.6176e-01,  1.4206e+00,  2.5502e+00,\n",
      "          6.3063e-01, -7.0751e-01,  1.5931e+00,  2.0236e+00, -1.9021e+00,\n",
      "         -1.6202e+00,  3.5347e+00, -1.2701e-01,  5.8850e-04,  2.4921e+00,\n",
      "          9.8536e-02,  8.4832e-01,  1.1682e+00,  1.6505e+00, -1.8983e+00,\n",
      "          1.2988e+00,  2.0655e-01, -3.3564e-01,  4.3634e-01, -1.2771e+00,\n",
      "         -1.0846e+00,  1.9912e+00,  7.3137e-01,  2.2675e+00,  4.3017e+00,\n",
      "         -1.0176e+00,  2.9612e-01,  7.0212e-01, -6.9218e-01,  1.6256e-01,\n",
      "          3.0140e+00,  4.5260e-01,  4.4974e-01,  4.8005e-01, -6.0075e-01,\n",
      "         -8.7548e-01, -1.1995e+00,  2.1568e-01,  1.7545e-01, -8.5829e-02,\n",
      "         -8.8549e-01, -1.8813e+00,  6.1226e-01, -3.3187e-01,  1.4646e+00,\n",
      "         -8.6473e-01,  2.3601e+00,  3.8449e-01, -4.2819e-02, -1.6297e+00,\n",
      "         -9.4091e-01, -8.2943e-01,  2.7528e-01,  1.3953e+00,  2.4129e+00,\n",
      "         -1.5250e+00, -3.3951e-01,  2.3785e+00,  1.2597e+00,  1.3931e+00,\n",
      "          1.4172e+00,  1.5486e+00,  4.6790e-01,  1.2407e+00,  1.7490e+00,\n",
      "          1.5283e+00,  1.2877e+00, -6.6016e-01,  9.7545e-01, -4.8053e-01,\n",
      "         -2.2084e+00,  7.6170e-01,  1.4712e+00, -1.5772e-01, -1.0264e+00,\n",
      "          1.3973e+00,  2.3551e+00, -3.2924e-01, -2.8262e-01,  1.8975e+00,\n",
      "          1.4995e+00,  2.7287e-01,  5.8927e-01, -3.0895e-01,  3.3920e-01,\n",
      "         -1.6316e+00, -1.8971e+00,  2.8328e-01,  9.4008e-01,  2.9294e-01,\n",
      "          1.4746e+00,  2.7027e+00,  8.9939e-01,  9.1537e-01,  2.5123e+00,\n",
      "         -4.9659e-01,  4.4905e-01, -1.6174e+00, -6.5195e-01,  1.2853e+00,\n",
      "          2.9492e+00, -1.2754e+00,  1.9991e+00,  2.2479e-01, -3.4678e-01,\n",
      "          1.5676e+00,  1.1708e-01,  3.3039e-02,  3.1116e-01, -2.4804e-01,\n",
      "         -6.5629e-01,  1.8890e+00,  6.2337e-01,  8.5974e-01, -1.1803e+00,\n",
      "         -2.3030e-01,  1.7993e+00,  1.2844e+00,  7.5931e-02,  2.4977e-02,\n",
      "          8.8424e-01,  3.7404e-01, -2.5881e-01,  3.2073e-01,  1.0460e+00,\n",
      "          1.3117e+00,  2.1120e-01,  1.1987e+00,  1.2129e-01,  2.2635e+00,\n",
      "         -6.2209e-01, -4.1939e-01,  1.7102e-01,  8.0685e-01,  1.5074e+00,\n",
      "          7.5961e-01, -5.6439e-01, -1.9623e+00,  1.4264e-01,  1.2815e+00,\n",
      "          1.3647e+00,  2.4280e+00, -1.7962e+00,  1.7991e+00,  1.1141e+00,\n",
      "         -5.5198e-02, -3.7225e-01,  2.2377e+00, -3.2027e-01,  2.3590e+00,\n",
      "          8.9382e-01,  2.2020e-01,  2.4114e+00,  1.5065e+00,  5.9616e-01,\n",
      "         -4.9917e-01,  2.6143e+00,  9.1444e-01,  1.1597e+00, -7.7182e-01,\n",
      "         -3.4065e-01,  3.0111e-01,  9.3476e-01,  2.0049e+00,  2.0506e+00,\n",
      "          1.1358e+00, -2.1960e-01,  1.2745e-01, -2.5425e+00,  1.4385e+00,\n",
      "         -3.4013e-01, -1.5026e-02,  1.0891e+00,  1.0228e-01, -7.8474e-01,\n",
      "         -7.3439e-02,  1.2307e+00, -1.1449e-01,  7.4160e-01,  4.3132e-01,\n",
      "          5.6866e-02, -7.1060e-01, -7.6899e-01, -1.1750e+00,  1.7109e+00,\n",
      "         -3.7204e-01,  9.1126e-01, -6.9148e-01, -1.5796e-01,  4.5960e-01,\n",
      "          1.6851e-01, -5.6349e-01,  1.5103e+00,  1.6821e+00, -6.3967e-01,\n",
      "         -5.8318e-01, -1.3063e+00, -6.4640e-01,  3.5918e+00,  1.0154e+00,\n",
      "         -1.3811e+00, -2.7561e-01,  4.5773e-01,  1.0997e+00, -8.3346e-02,\n",
      "         -5.0297e-01,  3.1913e+00,  9.2386e-01,  1.7636e+00, -1.3322e+00,\n",
      "         -1.4238e-01,  3.7252e-01,  8.6657e-02, -7.0898e-01,  1.2753e+00,\n",
      "         -5.3083e-01,  1.4104e+00,  1.9215e+00,  9.4550e-02,  9.7641e-01,\n",
      "          5.8489e-01,  6.0837e-01, -4.6675e-01, -3.1374e-01,  1.1647e+00,\n",
      "          3.0746e+00,  2.2718e+00, -9.1098e-01,  8.0359e-01,  6.5982e-01,\n",
      "          7.1419e-01,  1.6375e+00,  3.1239e+00, -1.1961e+00,  2.7806e+00,\n",
      "          2.1262e+00, -1.6961e+00,  6.7578e-01, -1.9560e-01,  3.2330e-01,\n",
      "         -2.8024e-01,  1.0298e+00,  1.9322e+00,  3.2235e-01, -8.2745e-01,\n",
      "         -2.4490e+00, -8.3473e-01,  2.7029e-02, -1.2279e-01,  3.6739e-02,\n",
      "         -7.3743e-01, -1.0052e+00,  1.1439e+00,  5.8653e-01, -2.2771e+00,\n",
      "          1.6957e+00,  2.8960e-01,  1.6519e-01,  2.7406e-01,  1.0243e+00,\n",
      "          6.8354e-01,  1.3051e+00,  1.4569e-02,  2.0671e+00, -3.8950e-01,\n",
      "          8.4777e-01,  5.9731e-01, -4.2981e-01,  3.9853e-01,  1.6082e+00,\n",
      "         -1.0562e+00, -6.5153e-01,  1.1589e+00,  1.0515e+00, -1.0797e-01,\n",
      "         -5.9010e-01,  1.4085e+00,  2.1836e-01,  1.7961e+00,  1.0557e+00,\n",
      "          2.8796e-01,  1.5785e+00,  2.2253e+00,  9.5454e-01,  4.0180e-01,\n",
      "          1.7378e+00,  1.5888e+00,  1.2028e+00,  9.3261e-01, -4.6363e-02,\n",
      "          5.6083e-01, -3.7991e-01, -2.0827e+00, -6.0302e-01, -1.6878e+00,\n",
      "         -1.5577e+00,  2.2901e+00, -4.7947e-01,  1.1453e-01,  8.9720e-01,\n",
      "          1.3781e+00, -1.6088e-01, -7.3373e-01, -6.0010e-01, -3.0706e-01,\n",
      "         -6.9982e-01, -1.0266e+00, -5.9497e-02, -3.6377e-01,  9.4681e-01,\n",
      "         -6.3669e-02, -7.5905e-01, -3.7355e-01, -8.4058e-01, -1.3182e-01,\n",
      "         -5.4144e-01, -2.1447e-01,  3.2557e-01,  2.8105e-01, -1.9552e+00,\n",
      "         -2.5531e-01, -1.4457e+00, -1.9068e-01, -6.6432e-02, -7.8428e-01,\n",
      "          6.2887e-01, -4.9362e-01, -6.9482e-01,  1.8514e+00,  1.2200e+00,\n",
      "          1.2500e+00,  1.3428e+00,  2.1305e-02,  7.0970e-02,  1.3205e+00,\n",
      "         -1.0824e+00,  2.5837e-01,  3.6670e-01,  6.2683e-02, -3.3401e-01,\n",
      "          6.0499e-03, -4.9059e-01, -6.5690e-01, -6.1182e-01, -5.1202e-01,\n",
      "         -5.8223e-01,  2.3824e+00,  1.3454e+00,  2.5291e+00,  9.3134e-01,\n",
      "         -9.7089e-01,  1.7340e+00, -1.0785e+00, -1.7956e+00,  2.5808e-01,\n",
      "         -5.8548e-01, -7.8750e-01, -1.3776e+00,  2.8363e-01, -1.0846e+00,\n",
      "          9.2817e-01,  1.4034e-01,  2.7596e-02,  2.7048e-01, -4.6318e-01,\n",
      "          1.5495e+00,  1.0480e-02,  1.5846e-01,  3.6647e-01, -9.9651e-01,\n",
      "          1.6610e-01, -1.0064e+00, -1.3880e+00, -1.7631e+00, -1.2975e+00,\n",
      "         -1.2445e+00, -1.8582e+00, -1.6173e+00,  2.1694e-04,  1.5588e+00]])\n"
     ]
    }
   ],
   "source": [
    "# this one really sucked\n",
    "\n",
    "from torchvision import models, transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Preprocess your input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "image_path = 'Frames/How Green Roofs Can Help Cities  NPR.mp4 || 0.jpg'\n",
    "image = Image.open(image_path)\n",
    "input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "# Print the results\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/hamdi/.venv/lib/python3.10/site-packages (4.38.1)\n",
      "Requirement already satisfied: filelock in /home/hamdi/.venv/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/hamdi/.venv/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/hamdi/.venv/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hamdi/.venv/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/hamdi/.venv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hamdi/.venv/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/hamdi/.venv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/hamdi/.venv/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/hamdi/.venv/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/hamdi/.venv/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hamdi/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hamdi/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hamdi/.venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hamdi/.venv/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hamdi/.venv/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hamdi/.venv/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /home/hamdi/.venv/lib/python3.10/site-packages (2.17.1)\n",
      "Requirement already satisfied: filelock in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/hamdi/.venv/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (0.21.2)\n",
      "Requirement already satisfied: packaging in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/hamdi/.venv/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/hamdi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/hamdi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hamdi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hamdi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/hamdi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/hamdi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hamdi/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hamdi/.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hamdi/.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hamdi/.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hamdi/.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hamdi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/hamdi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/hamdi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/hamdi/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: timm in /home/hamdi/.venv/lib/python3.10/site-packages (0.9.16)\n",
      "Requirement already satisfied: torch in /home/hamdi/.venv/lib/python3.10/site-packages (from timm) (2.2.0)\n",
      "Requirement already satisfied: torchvision in /home/hamdi/.venv/lib/python3.10/site-packages (from timm) (0.17.0)\n",
      "Requirement already satisfied: pyyaml in /home/hamdi/.venv/lib/python3.10/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/hamdi/.venv/lib/python3.10/site-packages (from timm) (0.21.2)\n",
      "Requirement already satisfied: safetensors in /home/hamdi/.venv/lib/python3.10/site-packages (from timm) (0.4.2)\n",
      "Requirement already satisfied: filelock in /home/hamdi/.venv/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hamdi/.venv/lib/python3.10/site-packages (from huggingface_hub->timm) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/hamdi/.venv/lib/python3.10/site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/hamdi/.venv/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hamdi/.venv/lib/python3.10/site-packages (from huggingface_hub->timm) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/hamdi/.venv/lib/python3.10/site-packages (from huggingface_hub->timm) (23.2)\n",
      "Requirement already satisfied: sympy in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/hamdi/.venv/lib/python3.10/site-packages (from torch->timm) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hamdi/.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.3.101)\n",
      "Requirement already satisfied: numpy in /home/hamdi/.venv/lib/python3.10/site-packages (from torchvision->timm) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/hamdi/.venv/lib/python3.10/site-packages (from torchvision->timm) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hamdi/.venv/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hamdi/.venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hamdi/.venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hamdi/.venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hamdi/.venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/hamdi/.venv/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"annotations/instances_train2017.json\", \"r\") as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440.jpg\n",
      "540.jpg\n",
      "180.jpg\n",
      "180.jpg\n",
      "5220.jpg\n",
      "3420.jpg\n",
      "3960.jpg\n",
      "1620.jpg\n",
      "5940.jpg\n",
      "1980.jpg\n",
      "540.jpg\n",
      "5400.jpg\n",
      "3420.jpg\n",
      "4500.jpg\n",
      "4860.jpg\n",
      "5400.jpg\n",
      "2340.jpg\n",
      "1980.jpg\n",
      "1800.jpg\n",
      "1260.jpg\n",
      "6480.jpg\n",
      "1260.jpg\n",
      "7560.jpg\n",
      "1080.jpg\n",
      "7740.jpg\n",
      "7380.jpg\n",
      "2340.jpg\n",
      "0.jpg\n",
      "0.jpg\n",
      "2700.jpg\n",
      "4500.jpg\n",
      "360.jpg\n",
      "4140.jpg\n",
      "5580.jpg\n",
      "2160.jpg\n",
      "4680.jpg\n",
      "2700.jpg\n",
      "3780.jpg\n",
      "6300.jpg\n",
      "5760.jpg\n",
      "2520.jpg\n",
      "3600.jpg\n",
      "720.jpg\n",
      "3240.jpg\n",
      "2340.jpg\n",
      "2880.jpg\n",
      "900.jpg\n",
      "1800.jpg\n",
      "360.jpg\n",
      "4320.jpg\n",
      "5220.jpg\n",
      "5040.jpg\n",
      "2880.jpg\n",
      "1260.jpg\n",
      "6660.jpg\n",
      "7920.jpg\n",
      "2160.jpg\n",
      "8100.jpg\n",
      "3960.jpg\n",
      "2160.jpg\n",
      "4140.jpg\n",
      "0.jpg\n",
      "4320.jpg\n",
      "180.jpg\n",
      "3060.jpg\n",
      "900.jpg\n",
      "3060.jpg\n",
      "2880.jpg\n",
      "1620.jpg\n",
      "4860.jpg\n",
      "6840.jpg\n",
      "6120.jpg\n",
      "1980.jpg\n",
      "4680.jpg\n",
      "1800.jpg\n",
      "1080.jpg\n",
      "3240.jpg\n",
      "3600.jpg\n",
      "2700.jpg\n",
      "3600.jpg\n",
      "1620.jpg\n",
      "900.jpg\n",
      "3780.jpg\n",
      "360.jpg\n",
      "3060.jpg\n",
      "720.jpg\n",
      "3420.jpg\n",
      "720.jpg\n",
      "1080.jpg\n",
      "540.jpg\n",
      "5040.jpg\n",
      "1440.jpg\n",
      "1440.jpg\n",
      "7020.jpg\n",
      "2520.jpg\n",
      "2520.jpg\n",
      "7200.jpg\n",
      "3240.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vidId</th>\n",
       "      <th>frameNum</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>detectedObjId</th>\n",
       "      <th>detectedObjClass</th>\n",
       "      <th>confidence</th>\n",
       "      <th>bbox info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>00:07.5:</td>\n",
       "      <td>1</td>\n",
       "      <td>person</td>\n",
       "      <td>0.631686</td>\n",
       "      <td>{'x1': 188.69049072265625, 'y1': 181.011657714...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>00:07.5:</td>\n",
       "      <td>44</td>\n",
       "      <td>bottle</td>\n",
       "      <td>0.575688</td>\n",
       "      <td>{'x1': 163.70094299316406, 'y1': 171.459213256...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>00:07.5:</td>\n",
       "      <td>51</td>\n",
       "      <td>bowl</td>\n",
       "      <td>0.124699</td>\n",
       "      <td>{'x1': 0.0, 'y1': 4.733412265777588, 'x2': 224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>00:07.5:</td>\n",
       "      <td>63</td>\n",
       "      <td>couch</td>\n",
       "      <td>0.120964</td>\n",
       "      <td>{'x1': 0.0, 'y1': 0.0, 'x2': 224.0, 'y2': 126....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>00:07.5:</td>\n",
       "      <td>3</td>\n",
       "      <td>car</td>\n",
       "      <td>0.093182</td>\n",
       "      <td>{'x1': 0.0, 'y1': 0.0, 'x2': 224.0, 'y2': 152....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>3</td>\n",
       "      <td>3240</td>\n",
       "      <td>02:15.0</td>\n",
       "      <td>44</td>\n",
       "      <td>bottle</td>\n",
       "      <td>0.142186</td>\n",
       "      <td>{'x1': 104.75902557373047, 'y1': 60.6594429016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>3</td>\n",
       "      <td>3240</td>\n",
       "      <td>02:15.0</td>\n",
       "      <td>47</td>\n",
       "      <td>cup</td>\n",
       "      <td>0.108981</td>\n",
       "      <td>{'x1': 60.953758239746094, 'y1': 54.2910804748...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>3</td>\n",
       "      <td>3240</td>\n",
       "      <td>02:15.0</td>\n",
       "      <td>44</td>\n",
       "      <td>bottle</td>\n",
       "      <td>0.071875</td>\n",
       "      <td>{'x1': 209.26283264160156, 'y1': 115.050064086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>3</td>\n",
       "      <td>3240</td>\n",
       "      <td>02:15.0</td>\n",
       "      <td>47</td>\n",
       "      <td>cup</td>\n",
       "      <td>0.067194</td>\n",
       "      <td>{'x1': 209.23802185058594, 'y1': 115.321105957...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>3</td>\n",
       "      <td>3240</td>\n",
       "      <td>02:15.0</td>\n",
       "      <td>44</td>\n",
       "      <td>bottle</td>\n",
       "      <td>0.063386</td>\n",
       "      <td>{'x1': 180.67208862304688, 'y1': 65.9109420776...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>554 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     vidId  frameNum timestamp  detectedObjId detectedObjClass  confidence  \\\n",
       "341      1       180  00:07.5:              1           person    0.631686   \n",
       "342      1       180  00:07.5:             44           bottle    0.575688   \n",
       "343      1       180  00:07.5:             51             bowl    0.124699   \n",
       "344      1       180  00:07.5:             63            couch    0.120964   \n",
       "345      1       180  00:07.5:              3              car    0.093182   \n",
       "..     ...       ...       ...            ...              ...         ...   \n",
       "268      3      3240   02:15.0             44           bottle    0.142186   \n",
       "269      3      3240   02:15.0             47              cup    0.108981   \n",
       "270      3      3240   02:15.0             44           bottle    0.071875   \n",
       "271      3      3240   02:15.0             47              cup    0.067194   \n",
       "272      3      3240   02:15.0             44           bottle    0.063386   \n",
       "\n",
       "                                             bbox info  \n",
       "341  {'x1': 188.69049072265625, 'y1': 181.011657714...  \n",
       "342  {'x1': 163.70094299316406, 'y1': 171.459213256...  \n",
       "343  {'x1': 0.0, 'y1': 4.733412265777588, 'x2': 224...  \n",
       "344  {'x1': 0.0, 'y1': 0.0, 'x2': 224.0, 'y2': 126....  \n",
       "345  {'x1': 0.0, 'y1': 0.0, 'x2': 224.0, 'y2': 152....  \n",
       "..                                                 ...  \n",
       "268  {'x1': 104.75902557373047, 'y1': 60.6594429016...  \n",
       "269  {'x1': 60.953758239746094, 'y1': 54.2910804748...  \n",
       "270  {'x1': 209.26283264160156, 'y1': 115.050064086...  \n",
       "271  {'x1': 209.23802185058594, 'y1': 115.321105957...  \n",
       "272  {'x1': 180.67208862304688, 'y1': 65.9109420776...  \n",
       "\n",
       "[554 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from PIL import Image\n",
    "\n",
    "tab_struct = pd.DataFrame(columns=[\"vidId\", \"frameNum\", \"timestamp\", \"detectedObjId\", \"detectedObjClass\", \"confidence\", \"bbox info\"])\n",
    "#load model\n",
    "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "#transform image\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "directory = 'Frames'\n",
    "for filename in os.listdir(directory):\n",
    "    image_path = os.path.join(directory, filename)\n",
    "\n",
    "    if not os.path.isfile(image_path):  # Ensure it's a file\n",
    "        continue\n",
    "    \n",
    "    #get vidid from name\n",
    "    vidId = 0\n",
    "    if filename.startswith(\"How\"):\n",
    "        vidId = 1\n",
    "    elif filename.startswith(\"What\"):\n",
    "        vidId = 2\n",
    "    elif filename.startswith(\"Why\"):\n",
    "        vidId = 3\n",
    "    \n",
    "    #get framenum from name\n",
    "    ind = filename.find(\"||\")\n",
    "    print(filename[ind+3:])\n",
    "    frameNum = int(filename[ind+3:].strip(\".jpg\"))\n",
    "\n",
    "    #calculating timestamp:\n",
    "    #downloaded videos are 23.98 fps, but I will round to 24\n",
    "    seconds = frameNum/24 #get time in pure seconds\n",
    "    hours = int(seconds // 60 // 60)\n",
    "    minutes = int((seconds // 60) % 60)\n",
    "    seconds = round(seconds % 60, 2) #round to 2 decimal places\n",
    "\n",
    "    timestamp = \"\" #will be in hh:mm:ss or mm:ss format\n",
    "    if hours > 0:\n",
    "        if hours < 10:\n",
    "            timestamp += f\"0{hours}:\"\n",
    "        else:\n",
    "            timestamp += f\"{hours}:\"\n",
    "    if minutes < 10:\n",
    "        timestamp += f\"0{minutes}:\"\n",
    "    else:\n",
    "        timestamp += f\"{minutes}:\"\n",
    "    if seconds < 10:\n",
    "        timestamp += f\"0{seconds}:\"\n",
    "    else:\n",
    "        timestamp += f\"{seconds}\"\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image)\n",
    "\n",
    "    image_tensor = image.unsqueeze(0)\n",
    "\n",
    "    #inference\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image_tensor)\n",
    "\n",
    "    category_mapping = {category['id']: category['name'] for category in annotations['categories']}\n",
    "\n",
    "    for box, label, score in zip(prediction[0]['boxes'], prediction[0]['labels'], prediction[0]['scores']):\n",
    "        # Extract bounding box coordinates\n",
    "        x1, y1, x2, y2 = box\n",
    "        x1 = x1.item()\n",
    "        y1 = y1.item()\n",
    "        x2 = x2.item()\n",
    "        y2 = y2.item()\n",
    "        \n",
    "        # Extract class label and map it to class name\n",
    "        class_id = label.item()\n",
    "        class_name = category_mapping[class_id]\n",
    "        \n",
    "        # Extract confidence score\n",
    "        confidence = score.item()\n",
    "        \n",
    "        # Print information\n",
    "        new_entry = {\"vidId\" : vidId, \"frameNum\": frameNum, \"timestamp\" : timestamp, \"detectedObjId\" : class_id, \"detectedObjClass\" : class_name, \"confidence\" : confidence, \"bbox info\" : {\"x1\": x1, \"y1\": y1, \"x2\" : x2, \"y2\": y2} }\n",
    "        tab_struct.loc[len(tab_struct.index)] = new_entry\n",
    "\n",
    "display(tab_struct.sort_values(by=[\"vidId\", \"frameNum\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.4041\n",
      "Epoch [2/20], Loss: 1.4213\n",
      "Epoch [3/20], Loss: 1.4049\n",
      "Epoch [4/20], Loss: 1.2322\n",
      "Epoch [5/20], Loss: 1.0141\n",
      "Epoch [6/20], Loss: 0.8601\n",
      "Epoch [7/20], Loss: 0.9113\n",
      "Epoch [8/20], Loss: 0.8811\n",
      "Epoch [9/20], Loss: 0.8011\n",
      "Epoch [10/20], Loss: 0.8557\n",
      "Epoch [11/20], Loss: 0.7049\n",
      "Epoch [12/20], Loss: 0.9893\n",
      "Epoch [13/20], Loss: 0.8891\n",
      "Epoch [14/20], Loss: 0.7289\n",
      "Epoch [15/20], Loss: 0.8901\n",
      "Epoch [16/20], Loss: 1.0747\n",
      "Epoch [17/20], Loss: 0.8068\n",
      "Epoch [18/20], Loss: 0.8427\n",
      "Epoch [19/20], Loss: 0.6554\n",
      "Epoch [20/20], Loss: 0.4224\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x, self.encoder(x) \n",
    "\n",
    "# Custom dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    #this dictionary will have to be modified depending on how the frame name is stored. This should work for windows OS\n",
    "    vidIdToName = {1: \"How Green Roofs Can Help Cities  NPR.mp4\", 2: \"What Does High-Quality Preschool Look Like  NPR Ed.mp4\", 3: \"Why It’s Usually Hotter In A City  Lets Talk  NPR.mp4\"}\n",
    "\n",
    "    def __init__(self, directory, tab_struct, transform):\n",
    "        self.directory = directory\n",
    "        self.tab_struct = tab_struct\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tab_struct)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_info = self.tab_struct.iloc[idx]\n",
    "        img_name = os.path.join(self.directory, f\"{self.vidIdToName[frame_info['vidId']]} || {frame_info['frameNum']}.jpg\")\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        # Crop the detected object from the frame using the bounding box coordinates\n",
    "        bbox = frame_info['bbox info']\n",
    "        object_image = image.crop((bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2']))\n",
    "        \n",
    "        object_image = self.transform(object_image)\n",
    "        return object_image\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "autoencoder.to(device)\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "dataset = ImageDataset(directory='Frames', tab_struct=tab_struct, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        # Move data to device\n",
    "        data = data.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _ = autoencoder(data)  # Extract the reconstruction output\n",
    "        reconstruction_loss = criterion(outputs, data)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = reconstruction_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss after each epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save the trained autoencoder model\n",
    "torch.save(autoencoder.state_dict(), 'autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# List to store the embeddings\u001b[39;00m\n\u001b[1;32m     39\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 41\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mpsycopg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhamdi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocalhost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8080\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#conn.execute('CREATE EXTENSION IF NOT EXISTS vector;')\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/psycopg/connection.py:748\u001b[0m, in \u001b[0;36mConnection.connect\u001b[0;34m(cls, conninfo, autocommit, prepare_threshold, row_factory, cursor_factory, context, **kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rv:\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m last_ex\n\u001b[0;32m--> 748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m last_ex\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    750\u001b[0m rv\u001b[38;5;241m.\u001b[39m_autocommit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(autocommit)\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row_factory:\n",
      "\u001b[0;31mOperationalError\u001b[0m: connection failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pgvector.psycopg import register_vector\n",
    "import psycopg\n",
    "import matplotlib.pyplot as plt\n",
    "from pgvector.psycopg import register_vector\n",
    "import psycopg\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib\n",
    "import os \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "matplotlib.use(\"TkAgg\")\n",
    "\n",
    "# conn = psycopg.connect(dbname='postgres', host=\"localhost\", port=5432)\n",
    "# conn.execute('CREATE EXTENSION IF NOT EXISTS vector;')\n",
    "# register_vector(conn)\n",
    "\n",
    "# curr = conn.cursor()\n",
    "\n",
    "def generate_embeddings(inputs):\n",
    "    inputs = inputs.view(1, 32, 32, 32)\n",
    "    return model(inputs.to(device)).detach().cpu().numpy()\n",
    "\n",
    "#Getting small vector embedding\n",
    "autoencoder.load_state_dict(torch.load('autoencoder.pth'))\n",
    "autoencoder.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder.to(device)\n",
    "\n",
    "dataset = ImageDataset(directory='Frames', tab_struct=tab_struct, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)  # Set batch size to 1 for inference\n",
    "\n",
    "# List to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "conn = psycopg.connect(dbname='postgres', user=\"postgres\", password='hamdi', host=\"localhost\", port=8080)\n",
    "print(\"connected\")\n",
    "#conn.execute('CREATE EXTENSION IF NOT EXISTS vector;')\n",
    "print(\"hello\")\n",
    "curr = conn.cursor()\n",
    "# Extract embeddings for each image\n",
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        _, embedding = autoencoder(data)\n",
    "        embeddings.append(embedding.cpu().numpy())  # Convert tensor to numpy array and move to CPU\n",
    "\n",
    "jack = embeddings\n",
    "# Convert the list of embeddings to a numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tried to make the vector database work but something was wrong with the size or something else and I could not get it to fit into that pgvector db as a tensor since the size was too different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(jack[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/554 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# generate, save, and index embeddings\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m tqdm(jack):\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#conn.execute('DROP TABLE IF EXISTS image;')\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#conn.execute('CREATE TABLE image (id bigserial PRIMARY KEY, embedding vector(1024));')\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[92], line 16\u001b[0m, in \u001b[0;36mgenerate_embeddings\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_embeddings\u001b[39m(inputs):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mautoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m, in \u001b[0;36mAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pgvector.psycopg import register_vector\n",
    "import psycopg\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib\n",
    "import os \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "matplotlib.use(\"TkAgg\")\n",
    "\n",
    "# tried and failed unfornutnely :(((((\n",
    "def generate_embeddings(inputs):\n",
    "    return autoencoder(inputs).detach().cpu().numpy()\n",
    "\n",
    "seed = True\n",
    "\n",
    "conn = psycopg.connect(dbname='postgres', user=\"postgres\", password='hamdi', host=\"localhost\", port=8080)\n",
    "print(\"connected\")\n",
    "#conn.execute('CREATE EXTENSION IF NOT EXISTS vector;')\n",
    "print(\"hello\")\n",
    "curr = conn.cursor()\n",
    "\n",
    "# generate, save, and index embeddings\n",
    "for img in tqdm(jack):\n",
    "    generate_embeddings(img)\n",
    "    #conn.execute('DROP TABLE IF EXISTS image;')\n",
    "    #conn.execute('CREATE TABLE image (id bigserial PRIMARY KEY, embedding vector(1024));')\n",
    "    count = 1\n",
    "    # print('Generating embeddings')\n",
    "    # for data in tqdm(data_loader):\n",
    "    #     embeddings = generate_embeddings(data[0])\n",
    "    #     np.save(f'img_embeddings/embeddings{count}.npy', embeddings)  \n",
    "    vectors_list = [tuple(vector) for vector in img]\n",
    "    conn.execute('INSERT INTO img (embedding) VALUES (%t)', (vectors_list,) )\n",
    "    count+=1\n",
    "    print(count)\n",
    "conn.commit()\n",
    "\n",
    "#for image, embedding in zip(images, embeddings):\n",
    "result = conn.execute('SELECT id FROM image ORDER BY embedding <=> %s LIMIT 15;', (embedding[0],)).fetchall()\n",
    "print(\"Found: \", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
